{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EJxsj34cIfqKsb9f1E6sm1-dt4P2qx98",
      "authorship_tag": "ABX9TyMcWhG+ZMCtfD2cTyoS+86K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarehWilliams01/nlp/blob/main/Natural_Language_Processing_With_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "id": "uzwZJ2auAcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm9J8z2Bzw4h"
      },
      "outputs": [],
      "source": [
        "# Importing the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the test and train dataset\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Datasets/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Datasets/test.csv')\n",
        "\n",
        "remove = ['id', 'keyword', 'location']\n",
        "\n",
        "df_train = df_train.drop(remove, axis=1)\n",
        "df_test = df_test.drop(remove, axis=1)\n",
        "\n",
        "df_train\n",
        "df_test"
      ],
      "metadata": {
        "id": "I75QeIsD8suU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Developing a dictionary for shorthand texts\n",
        "# Sending A GET request\n",
        "url = 'https://messente.com/blog/text-abbreviations'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parsing the HTML content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "#Extracting the data from the second and third 'b' tags in the html page\n",
        "b_tags = soup.find_all('b')\n",
        "second_b_tag = b_tags[1]\n",
        "third_b_tag = b_tags[2]\n",
        "\n",
        "#Finding all but the first paragraph in the second and the third b tags\n",
        "second_paragraphs = second_b_tag('p')[1:]\n",
        "third_paragraphs = third_b_tag('p')[1:]\n",
        "\n",
        "#Extracting the slangs with their respective descriptions and storing it in a dataframe\n",
        "slang_list = []\n",
        "description_list = []\n",
        "count = 0\n",
        "\n",
        "for paragraph in second_paragraphs + third_paragraphs:\n",
        "  if count <= 99:\n",
        "    split_text = paragraph.text.strip().split(\" â€“ \", 1)\n",
        "    slang = split_text[0].split(\". \", 1)[-1].lower()\n",
        "    description = split_text[1] if len(split_text) > 1 else \"\"\n",
        "    slang_list.append(slang)\n",
        "    description_list.append(description)\n",
        "    count += 1\n",
        "\n",
        "  else:\n",
        "    break\n",
        "\n",
        "df_slangs = pd.DataFrame({\n",
        "    \"slang\": slang_list,\n",
        "    \"description\": description_list\n",
        "})\n",
        "\n",
        "df_slangs"
      ],
      "metadata": {
        "id": "-TaaUfoaAac4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the data\n",
        "\n",
        "# Developing a function to clean the data\n",
        "def cleaning_tweet(text):\n",
        "  text = re.sub(r'@[A-Za-z0-9]+', '', text) # removes mentions\n",
        "  text = re.sub(r'#', '', text) # removes hashtags\n",
        "  text = re.sub(r'RT[\\s]+', '', text) # removes retweets\n",
        "  text = re.sub(r'https?:\\/\\/\\S+', '', text) # removes hyperlinks\n",
        "  text = re.sub(r'\\.', '.', text) # removes repeated fullstops\n",
        "  text = re.sub(r'!', '', text) # removes repeated exclammation marks\n",
        "  text = re.sub(r'\\?', '', text) # removes repeated question marks\n",
        "  text = re.sub(r'\\s+', ' ', text) # removes extra space around text\n",
        "  text = emoji.demojize(text, delimiters=(\"\", \"\")) # replacing the emojis with respective labels\n",
        "  # removes slangs\n",
        "  words = text.split() # splits text into each word\n",
        "  normalized_words = [df_slangs.loc[df_slangs['slang'] == word, \"description\"].values[0]\n",
        "    if word in df_slangs['slang'].values else word for word in words]\n",
        "  text = \" \".join(normalized_words)\n",
        "\n",
        "  return text.lower()\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(cleaning_tweet)\n",
        "df_test['text'] = df_test['text'].apply(cleaning_tweet)"
      ],
      "metadata": {
        "id": "cvG1agTp-1QJ"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to lemmatize each word in a tweet\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(lemmatize_text)\n",
        "df_test['text'] = df_test['text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "29CFNr0YCmna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "\n",
        "# Getting the current list of English stopwords\n",
        "stopword_list = stopwords.words('english')\n",
        "\n",
        "# Function to remove stopwords from tweet\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "# Apply stopword removal to the 'tweet' column\n",
        "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
        "\n",
        "df_train"
      ],
      "metadata": {
        "id": "UelWZr0DifMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Vectors\n",
        "count_vectorizer = CountVectorizer()\n",
        "train_vectors = count_vectorizer.fit_transform(df_train['text'])\n",
        "test_vectors = count_vectorizer.transform(df_test['text'])\n",
        "\n",
        "train_vectors\n",
        "test_vectors\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_train = tfidf_transformer.fit_transform(train_vectors)\n",
        "tfidf_test = tfidf_transformer.transform(test_vectors)"
      ],
      "metadata": {
        "id": "jYuwLO6xRj-4"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Our Model\n",
        "labels = df_train['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_train, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model1: MultinomialNB\n",
        "\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Metrics\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_precision = precision_score(y_train, train_predictions)\n",
        "train_recall = recall_score(y_train, train_predictions)\n",
        "train_f1 = f1_score(y_train, train_predictions)\n",
        "\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "print(train_accuracy, train_precision, train_recall, train_f1)\n",
        "print( test_accuracy, test_precision, test_recall, test_f1)\n"
      ],
      "metadata": {
        "id": "Yzco7xJIcMxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model2: Ridge Classifier\n",
        "\n",
        "model = RidgeClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Metrics\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_precision = precision_score(y_train, train_predictions)\n",
        "train_recall = recall_score(y_train, train_predictions)\n",
        "train_f1 = f1_score(y_train, train_predictions)\n",
        "\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "print(train_accuracy, train_precision, train_recall, train_f1)\n",
        "print( test_accuracy, test_precision, test_recall, test_f1)"
      ],
      "metadata": {
        "id": "eVXedO6K6Tnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model3: SVC\n",
        "\n",
        "model = svm.LinearSVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Metrics\n",
        "\n",
        "train_predictions = model.predict(X_train)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "train_precision = precision_score(y_train, train_predictions)\n",
        "train_recall = recall_score(y_train, train_predictions)\n",
        "train_f1 = f1_score(y_train, train_predictions)\n",
        "\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_precision = precision_score(y_test, test_predictions)\n",
        "test_recall = recall_score(y_test, test_predictions)\n",
        "test_f1 = f1_score(y_test, test_predictions)\n",
        "\n",
        "print(train_accuracy, train_precision, train_recall, train_f1)\n",
        "print( test_accuracy, test_precision, test_recall, test_f1)"
      ],
      "metadata": {
        "id": "XTkiXEvHDdWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting sample submission values and storing them\n",
        "\n",
        "y_train_t = df_train['target']\n",
        "model.fit(tfidf_train, y_train_t)\n",
        "pred = model.predict(tfidf_test)\n",
        "\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Datasets/sample_submission.csv')\n",
        "sample_submission['target'] = pred\n",
        "\n",
        "sample_submission\n",
        "\n",
        "sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "9MiFvXaTm-dc"
      },
      "execution_count": 184,
      "outputs": []
    }
  ]
}