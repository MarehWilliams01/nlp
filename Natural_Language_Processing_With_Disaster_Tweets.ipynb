{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "id": "uzwZJ2auAcx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qm9J8z2Bzw4h",
        "outputId": "3d6851ae-6461-410c-aeef-f4dacd8b301f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "# Importing the neccessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the test and train dataset\n",
        "df_train = pd.read_csv('/content/drive/MyDrive/Datasets/train.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/Datasets/test.csv')\n",
        "\n",
        "remove = ['id', 'keyword', 'location']\n",
        "\n",
        "df_train = df_train.drop(remove, axis=1)\n",
        "df_test = df_test.drop(remove, axis=1)\n",
        "\n",
        "df_train\n",
        "df_test"
      ],
      "metadata": {
        "id": "I75QeIsD8suU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Developing a dictionary for shorthand texts\n",
        "# Sending A GET request\n",
        "url = 'https://messente.com/blog/text-abbreviations'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Parsing the HTML content with BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "#Extracting the data from the second and third 'b' tags in the html page\n",
        "b_tags = soup.find_all('b')\n",
        "second_b_tag = b_tags[1]\n",
        "third_b_tag = b_tags[2]\n",
        "\n",
        "#Finding all but the first paragraph in the second and the third b tags\n",
        "second_paragraphs = second_b_tag('p')[1:]\n",
        "third_paragraphs = third_b_tag('p')[1:]\n",
        "\n",
        "#Extracting the slangs with their respective descriptions and storing it in a dataframe\n",
        "slang_list = []\n",
        "description_list = []\n",
        "count = 0\n",
        "\n",
        "for paragraph in second_paragraphs + third_paragraphs:\n",
        "  if count <= 99:\n",
        "    split_text = paragraph.text.strip().split(\" â€“ \", 1)\n",
        "    slang = split_text[0].split(\". \", 1)[-1].lower()\n",
        "    description = split_text[1] if len(split_text) > 1 else \"\"\n",
        "    slang_list.append(slang)\n",
        "    description_list.append(description)\n",
        "    count += 1\n",
        "\n",
        "  else:\n",
        "    break\n",
        "\n",
        "df_slangs = pd.DataFrame({\n",
        "    \"slang\": slang_list,\n",
        "    \"description\": description_list\n",
        "})\n",
        "\n",
        "df_slangs"
      ],
      "metadata": {
        "id": "-TaaUfoaAac4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing the data\n",
        "\n",
        "# Developing a function to clean the data\n",
        "def cleaning_tweet(text):\n",
        "  text = re.sub(r'@[A-Za-z0-9]+', '', text) # removes mentions\n",
        "  text = re.sub(r'#', '', text) # removes hashtags\n",
        "  text = re.sub(r'RT[\\s]+', '', text) # removes retweets\n",
        "  text = re.sub(r'https?:\\/\\/\\S+', '', text) # removes hyperlinks\n",
        "  text = re.sub(r'\\.', '.', text) # removes repeated fullstops\n",
        "  text = re.sub(r'!', '', text) # removes repeated exclammation marks\n",
        "  text = re.sub(r'\\?', '', text) # removes repeated question marks\n",
        "  text = re.sub(r'\\s+', ' ', text) # removes extra space around text\n",
        "  text = emoji.demojize(text, delimiters=(\"\", \"\")) # replacing the emojis with respective labels\n",
        "  # removes slangs\n",
        "  words = text.split() # splits text into each word\n",
        "  normalized_words = [df_slangs.loc[df_slangs['slang'] == word, \"description\"].values[0]\n",
        "    if word in df_slangs['slang'].values else word for word in words]\n",
        "  text = \" \".join(normalized_words)\n",
        "\n",
        "  return text.lower()\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(cleaning_tweet)\n",
        "df_test['text'] = df_test['text'].apply(cleaning_tweet)"
      ],
      "metadata": {
        "id": "cvG1agTp-1QJ"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to lemmatize each word in a tweet\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return ' '.join(lemmas)\n",
        "\n",
        "df_train['text'] = df_train['text'].apply(lemmatize_text)\n",
        "df_test['text'] = df_test['text'].apply(lemmatize_text)"
      ],
      "metadata": {
        "id": "29CFNr0YCmna"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords\n",
        "\n",
        "# Getting the current list of English stopwords\n",
        "stopword_list = stopwords.words('english')\n",
        "\n",
        "# Function to remove stopwords from tweet\n",
        "def remove_stopwords(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    text = ' '.join(filtered_tokens)\n",
        "    return text\n",
        "\n",
        "# Apply stopword removal to the 'tweet' column\n",
        "df_train['text'] = df_train['text'].apply(remove_stopwords)\n",
        "\n",
        "df_train"
      ],
      "metadata": {
        "id": "UelWZr0DifMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Vectors\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "train_vectors = count_vectorizer.fit_transform(df_train['text'])\n",
        "test_vectors = count_vectorizer.transform(df_test['text'])\n",
        "\n",
        "train_vectors\n",
        "test_vectors\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "tfidf_train = tfidf_transformer.fit_transform(train_vectors)\n",
        "tfidf_test = tfidf_transformer.transform(test_vectors)"
      ],
      "metadata": {
        "id": "jYuwLO6xRj-4"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Our Model\n",
        "\n",
        "model = MultinomialNB()\n",
        "y_train = df_train['target']\n",
        "scores = cross_val_score(model, tfidf_train, df_train[\"target\"], cv=3, scoring=\"f1\")\n",
        "scores"
      ],
      "metadata": {
        "id": "Yzco7xJIcMxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting sample submission values and storing them\n",
        "\n",
        "model.fit(tfidf_train, y_train)\n",
        "pred = model.predict(tfidf_test)\n",
        "\n",
        "sample_submission = pd.read_csv('/content/drive/MyDrive/Datasets/sample_submission.csv')\n",
        "sample_submission['target'] = pred\n",
        "\n",
        "sample_submission\n",
        "\n",
        "sample_submission.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "9MiFvXaTm-dc"
      },
      "execution_count": 145,
      "outputs": []
    }
  ]
}